{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2104f228-084e-4196-8f24-76f96d7ac654",
   "metadata": {},
   "source": [
    "#### Binary sentiment prediction using transformer-based models:\n",
    "- **Objective**: Predict whether a movie review is negative (0) or positive (1).\n",
    "- **Approach**:\n",
    "  1. **Step 1**: Take a pre-trained transformer model and convert textual data into numerical representations (embeddings).\n",
    "  2. **Step 2**: Fine-tune the model on labeled data (e.g., reviews with positive/negative labels) to perform the classification task.\n",
    "  \n",
    "- this process begins by transforming text into numerical form using transformer-based architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c51b1e-e08a-45b5-a7d3-67e6c7458b21",
   "metadata": {},
   "source": [
    "#### Using the Rotten Tomatoes dataset for binary classification:\n",
    "- **Task**: Binary classification, a common predictive task.\n",
    "- **Application**: Sentiment analysis.\n",
    "- **Objective**: Detect whether a document (e.g., review) is positive or negative.\n",
    "- **Dataset**: Contains customer reviews labeled as either positive or negative (binary labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f004679a-8f84-46cc-847c-e67536859c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f5e966-d800-46e1-8e1a-fbdc5f6db8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('cornell-movie-review-data/rotten_tomatoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0972abc7-0860-4c2f-9a9e-d9bf515a44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d15b67-5aeb-49a0-8a27-800c5131406d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8530, 2), (1066, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas for easier control\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "eval_df  = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "train_df.shape, eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78026b7-6d5d-4468-9c65-cae54369baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9a749-b5ca-4c23-8b40-d16e88143850",
   "metadata": {},
   "source": [
    "#### Training a classifier with a transformer-based model:\n",
    "\n",
    "- **Step 1**: Use a pre-trained Large Language Model (LLM), such as BERT, to convert textual data into numerical representations (embeddings).\n",
    "  \n",
    "  - **Optimization**: The LLM's weights are \"frozen\" during training to speed up the process, but this may reduce accuracy.\n",
    "\n",
    "- **Step 2**: Add a classification head on top of the pre-trained model.\n",
    "  - A single linear layer (classification head) is placed on top, which is fine-tuned for the binary classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d8b74-fc23-49d5-96f2-b8f005aa73ea",
   "metadata": {},
   "source": [
    "- The `simpletransformers` library is an easy-to-use wrapper around the Hugging Face Transformers library, designed to simplify the process of training and fine-tuning transformer models.\n",
    "  \n",
    "- It abstracts many of the complexities of working with transformer models, making it a great choice for users who want to quickly train models for various NLP tasks without diving into too much detail.\n",
    "  \n",
    "- simpletransformers supports several common NLP tasks, including:\n",
    "\n",
    "    - Text classification (binary, multi-label, and multi-class)\n",
    "    - Named entity recognition (NER)\n",
    "    - Question-answering (QA)\n",
    "    - Language modeling\n",
    "    - Sequence-to-sequence tasks (e.g., translation, summarization)\n",
    "    - Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ba7938-f5bd-4553-8db0-8e08defef907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd4261a9-e26b-447a-b984-eac15f0466a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare your data\n",
    "# Data should be in a DataFrame with two columns: \"text\" and \"labels\"\n",
    "train_data = [\n",
    "    [\"I love transformers!\", 1],\n",
    "    [\"Transformers are challenging.\", 0],\n",
    "    [\"I enjoy learning about NLP.\", 1],\n",
    "    [\"Sometimes transformers are difficult.\", 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2fc914-9bc3-49de-8f4b-b07aa6ae94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_data, columns=[\"text\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcf79fc9-2fe3-47fe-b2e0-2f1a86906bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\ANACONDA\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# For this example, we'll use 'roberta' model for binary classification\n",
    "# model_type: The type of model (bert, xlnet, xlm, roberta, distilbert)\n",
    "# model_name: The exact architecture and trained weights to use. \n",
    "#             This may be a Hugging Face Transformers compatible pre-trained model, \n",
    "#             a community model, or the path to a directory containing model files.\n",
    "# tokenizer_type: The type of tokenizer \n",
    "#             (auto, bert, xlnet, xlm, roberta, distilbert, etc.)\n",
    "\n",
    "# Specify model and tokenizer type\n",
    "model_type     = \"roberta\"\n",
    "model_name     = \"roberta-base\"\n",
    "tokenizer_type = \"roberta\"\n",
    "\n",
    "model = ClassificationModel(\n",
    "    model_type    = model_type,\n",
    "    model_name    = model_name,\n",
    "    tokenizer_type= tokenizer_type,\n",
    "    use_cuda      = False,\n",
    "    args={\n",
    "            'overwrite_output_dir': True\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f377976b-5d99-42f2-a66a-0b2ae38f97bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb21d0a65ab84ab8ae5b7f65d7c1ef66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e593c358fd0a4a2ba03a8c1372f71ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b44edf95a314c2aba13239094e077b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model with your data\n",
    "global_step, training_details = model.train_model(df,\n",
    "                                                show_running_loss       = True,\n",
    "                                                evaluate_during_training= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4403829-7eae-4fb0-986c-54d79232d5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Steps: 1\n",
      "Training Details: 0.7056629657745361\n"
     ]
    }
   ],
   "source": [
    "# Output the results\n",
    "print(f\"Global Steps: {global_step}\")\n",
    "print(f\"Training Details: {training_details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b934fa0-bd57-4936-a142-d8bb9a7aea3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0869fe3ac34a5babdd5d548db62ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fad46ec49b4835bd8d4b39a16b63bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[ 0.05071136 -0.16328129]]\n"
     ]
    }
   ],
   "source": [
    "# Sample input for prediction\n",
    "test_texts = [\"Weather is very bad today\"]\n",
    "\n",
    "# Make predictions\n",
    "predictions, probabilities = model.predict(test_texts)\n",
    "\n",
    "# Display predictions\n",
    "print(predictions)     # This might print: [1]\n",
    "print(probabilities)   # This might print: [0.7084888219833374]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ce68b-e8c6-4de0-bbb6-21b211872561",
   "metadata": {},
   "source": [
    "... back to original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac47338-35d7-41c4-b599-6268d0d7fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002fdae2-80a6-4c58-a75f-4cea79d740fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6344</th>\n",
       "      <td>imagine a really bad community theater production of west side story without the songs .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8354</th>\n",
       "      <td>this sort of cute and cloying material is far from zhang's forte and it shows .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3388</th>\n",
       "      <td>we've seen it all before in one form or another , but director hoffman , with great help from kevin kline , makes us care about this latest reincarnation of the world's greatest teacher .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>'no es la mejor cinta de la serie , ni la mejor con brosnan a la cabeza , pero de que entretiene ni duda cabe . '</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5359</th>\n",
       "      <td>less worrying about covering all the drama in frida's life and more time spent exploring her process of turning pain into art would have made this a superior movie .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                             text  \\\n",
       "6344                                                                                                     imagine a really bad community theater production of west side story without the songs .   \n",
       "8354                                                                                                              this sort of cute and cloying material is far from zhang's forte and it shows .   \n",
       "3388  we've seen it all before in one form or another , but director hoffman , with great help from kevin kline , makes us care about this latest reincarnation of the world's greatest teacher .   \n",
       "1586                                                                            'no es la mejor cinta de la serie , ni la mejor con brosnan a la cabeza , pero de que entretiene ni duda cabe . '   \n",
       "5359                        less worrying about covering all the drama in frida's life and more time spent exploring her process of turning pain into art would have made this a superior movie .   \n",
       "\n",
       "      label  \n",
       "6344      0  \n",
       "8354      0  \n",
       "3388      1  \n",
       "1586      1  \n",
       "5359      0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1db31366-5b3b-490b-8bb6-82c3f921b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model arguments\n",
    "model_args = ClassificationArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2449f26b-f0ca-46b2-8991-644825afe1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simpletransformers.config.model_args.ClassificationArgs"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "500f7a40-8bc7-42b1-996c-9fde386faa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationArgs(adafactor_beta1=None,\n",
      "                   adafactor_clip_threshold=1.0,\n",
      "                   adafactor_decay_rate=-0.8,\n",
      "                   adafactor_eps=(1e-30, 0.001),\n",
      "                   adafactor_relative_step=True,\n",
      "                   adafactor_scale_parameter=True,\n",
      "                   adafactor_warmup_init=True,\n",
      "                   adam_betas=(0.9, 0.999),\n",
      "                   adam_epsilon=1e-08,\n",
      "                   best_model_dir='outputs/best_model',\n",
      "                   cache_dir='cache_dir/',\n",
      "                   config={},\n",
      "                   cosine_schedule_num_cycles=0.5,\n",
      "                   custom_layer_parameters=[],\n",
      "                   custom_parameter_groups=[],\n",
      "                   dataloader_num_workers=0,\n",
      "                   do_lower_case=False,\n",
      "                   dynamic_quantize=False,\n",
      "                   early_stopping_consider_epochs=False,\n",
      "                   early_stopping_delta=0,\n",
      "                   early_stopping_metric='eval_loss',\n",
      "                   early_stopping_metric_minimize=True,\n",
      "                   early_stopping_patience=3,\n",
      "                   encoding=None,\n",
      "                   eval_batch_size=100,\n",
      "                   evaluate_during_training=False,\n",
      "                   evaluate_during_training_silent=True,\n",
      "                   evaluate_during_training_steps=2000,\n",
      "                   evaluate_during_training_verbose=False,\n",
      "                   evaluate_each_epoch=True,\n",
      "                   fp16=True,\n",
      "                   gradient_accumulation_steps=1,\n",
      "                   learning_rate=4e-05,\n",
      "                   local_rank=-1,\n",
      "                   logging_steps=50,\n",
      "                   loss_type=None,\n",
      "                   loss_args={},\n",
      "                   manual_seed=None,\n",
      "                   max_grad_norm=1.0,\n",
      "                   max_seq_length=128,\n",
      "                   model_name=None,\n",
      "                   model_type=None,\n",
      "                   multiprocessing_chunksize=-1,\n",
      "                   n_gpu=1,\n",
      "                   no_cache=False,\n",
      "                   no_save=False,\n",
      "                   not_saved_args=[],\n",
      "                   num_train_epochs=1,\n",
      "                   optimizer='AdamW',\n",
      "                   output_dir='outputs/',\n",
      "                   overwrite_output_dir=False,\n",
      "                   polynomial_decay_schedule_lr_end=1e-07,\n",
      "                   polynomial_decay_schedule_power=1.0,\n",
      "                   process_count=2,\n",
      "                   quantized_model=False,\n",
      "                   reprocess_input_data=True,\n",
      "                   save_best_model=True,\n",
      "                   save_eval_checkpoints=True,\n",
      "                   save_model_every_epoch=True,\n",
      "                   save_optimizer_and_scheduler=True,\n",
      "                   save_steps=2000,\n",
      "                   scheduler='linear_schedule_with_warmup',\n",
      "                   silent=False,\n",
      "                   skip_special_tokens=True,\n",
      "                   tensorboard_dir=None,\n",
      "                   thread_count=None,\n",
      "                   tokenizer_name=None,\n",
      "                   tokenizer_type=None,\n",
      "                   train_batch_size=8,\n",
      "                   train_custom_parameters_only=False,\n",
      "                   trust_remote_code=False,\n",
      "                   use_cached_eval_features=False,\n",
      "                   use_early_stopping=False,\n",
      "                   use_hf_datasets=False,\n",
      "                   use_multiprocessing=True,\n",
      "                   use_multiprocessing_for_evaluation=True,\n",
      "                   wandb_kwargs={},\n",
      "                   wandb_project=None,\n",
      "                   warmup_ratio=0.06,\n",
      "                   warmup_steps=0,\n",
      "                   weight_decay=0.0,\n",
      "                   model_class='ClassificationModel',\n",
      "                   labels_list=[],\n",
      "                   labels_map={},\n",
      "                   lazy_delimiter='\\t',\n",
      "                   lazy_labels_column=1,\n",
      "                   lazy_loading=False,\n",
      "                   lazy_loading_start_line=1,\n",
      "                   lazy_text_a_column=None,\n",
      "                   lazy_text_b_column=None,\n",
      "                   lazy_text_column=0,\n",
      "                   onnx=False,\n",
      "                   regression=False,\n",
      "                   sliding_window=False,\n",
      "                   special_tokens_list=[],\n",
      "                   stride=0.8,\n",
      "                   tie_value=1)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f99a82a-ed07-4704-8208-06f7ab635e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.train_custom_parameters_only = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b823b-c0e8-4cb8-a297-0178ccab8699",
   "metadata": {},
   "source": [
    "#### `train_custom_parameters_only` Parameter Explanation\n",
    "\n",
    "- **Default Value:** `False`\n",
    "- **Purpose:** Controls whether only the custom (newly added) layers are trained during fine-tuning, or if the entire model (including the base model) is trained.\n",
    "\n",
    "---\n",
    "\n",
    "##### When `train_custom_parameters_only = True`\n",
    "- **What happens:**\n",
    "  - Only the custom layers you’ve added on top of the RoBERTa model (e.g., the classification head) are trained.\n",
    "  - The **pre-trained RoBERTa layers are frozen** and their weights are not updated during training.\n",
    "\n",
    "- **Use Case:**\n",
    "  - Useful when you want **quick training** or **fine-tuning** with limited data or computational resources.\n",
    "  - You don't modify the pre-trained RoBERTa layers, but adapt the model by training only the classifier (or custom layers) for your dataset.\n",
    "\n",
    "---\n",
    "\n",
    "##### When `train_custom_parameters_only = False` (default)\n",
    "- **What happens:**\n",
    "  - The entire model, including both the pre-trained RoBERTa layers and the custom layers, is trained.\n",
    "  - This means all parameters are updated during the fine-tuning process.\n",
    "\n",
    "- **Use Case:**\n",
    "  - Used when you want to **fine-tune the entire model** to fully adapt RoBERTa to the nuances of your specific dataset.\n",
    "  - This is the typical setting for most fine-tuning tasks where you adjust both the base model and the custom layers for better task-specific performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d046eac-25c9-4ca4-94f3-68e2d39c6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.custom_parameter_groups = [\n",
    "    {\n",
    "        \"params\": [\"classifier.weight\"],\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\"classifier.bias\"],\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e4530-1831-4e35-aebf-c9aced1d2806",
   "metadata": {},
   "source": [
    "#### custom_parameter_groups\n",
    "\n",
    "- **Type:** List of dictionaries\n",
    "- **Purpose:** Allows you to define custom parameter groups for fine-tuning specific parts of the model, assigning them different learning rates, weight decays, or other settings during optimization.\n",
    "\n",
    "---\n",
    "\n",
    "##### Structure of `custom_parameter_groups`:\n",
    "\n",
    "Each dictionary in the list can include:\n",
    "- **`params`:** The list of parameters or parameter layers to which the settings will apply.\n",
    "- **`lr`:** Learning rate for the specific parameter group.\n",
    "- **`weight_decay`:** Weight decay for the specific group.\n",
    "- You can also include other optimizer-specific settings.\n",
    "\n",
    "---\n",
    "\n",
    "##### Example Usage:\n",
    "\n",
    "```python\n",
    "custom_parameter_groups = [\n",
    "    {\n",
    "        \"params\": [\"roberta.embeddings.*\"],      # Parameter group for the embedding layer\n",
    "        \"lr\": 5e-5,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\"roberta.encoder.layer.0.*\"], # Parameter group for encoder layer 0\n",
    "        \"lr\": 4e-5,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\"classifier.*\"],              # Parameter group for the custom classification head\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 0.0\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c460d0-bcd0-45d0-aec1-03f24ef5994f",
   "metadata": {},
   "source": [
    "| **Name of the Layer**             | **Value**                                      | **Description**                                         |\n",
    "|-----------------------------------|------------------------------------------------|---------------------------------------------------------|\n",
    "| **Embedding Layers**              | `\"roberta.embeddings.*\"`                       | Parameters for the embedding layers                     |\n",
    "| **Transformer Encoder Layers**    | `\"roberta.encoder.layer.*\"`                    | Parameters for all transformer encoder layers           |\n",
    "| **Specific Encoder Layers**       | `\"roberta.encoder.layer.0.*\"`                  | Parameters for specific encoder layer (e.g., layer 0)   |\n",
    "| **Attention Layers**              | `\"roberta.encoder.layer.[N].attention.*\"`      | Parameters for the attention sub-layer in layer N       |\n",
    "| **Feedforward Layers**            | `\"roberta.encoder.layer.[N].intermediate.*\"`   | Parameters for the intermediate feedforward layer in N  |\n",
    "| **Output Layers**                 | `\"roberta.encoder.layer.[N].output.*\"`         | Parameters for the output layer in transformer block N  |\n",
    "| **Layer Normalization Layers**    | `\"roberta.encoder.layer.[N].output.LayerNorm.*\"` | Parameters for the LayerNorm in the output sub-layer of N |\n",
    "| **Pooler Layer**                  | `\"roberta.pooler.*\"`                           | Parameters for the pooler layer                         |\n",
    "| **Task-Specific Layers**          | `\"classifier.*\"`                               | Parameters for the task-specific classification head    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3975c39-ce32-4313-8a52-535b846c62b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\ANACONDA\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Specify model and tokenizer type\n",
    "model_type     = \"bert\"\n",
    "model_name     = \"bert-base-cased\"\n",
    "tokenizer_type = \"bert\"\n",
    "\n",
    "model = ClassificationModel(\n",
    "    model_type    = model_type,\n",
    "    model_name    = model_name,\n",
    "    tokenizer_type= tokenizer_type,\n",
    "    use_cuda      = False,\n",
    "    args={\n",
    "            'overwrite_output_dir': True\n",
    "        }\n",
    ")\n",
    "\n",
    "model = ClassificationModel(\n",
    "    model_type    = model_type,\n",
    "    model_name    = model_name,\n",
    "    tokenizer_type= tokenizer_type,\n",
    "    use_cuda      = False,\n",
    "    args={\n",
    "            'custom_layer_parameters': [\n",
    "                {\"layer\": 0, \"type\": \"Dense\", \"units\": 128, \"activation\": \"relu\"},  # First layer\n",
    "                {\"layer\": 1, \"type\": \"Dropout\", \"rate\": 0.3},  # Dropout layer\n",
    "                {\"layer\": 2, \"type\": \"Dense\", \"units\": 64, \"activation\": \"relu\"},  # Second layer\n",
    "                {\"layer\": 3, \"type\": \"Dense\", \"units\": 1, \"activation\": \"sigmoid\"},  # Output layer\n",
    "            ],\n",
    "            'overwrite_output_dir': True,\n",
    "            'num_train_epochs': 3,  # Number of epochs for training\n",
    "            'train_batch_size': 16,  # Batch size for training\n",
    "            'learning_rate': 5e-5,  # Learning rate\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db86fda1-9bd4-49a9-9361-f850cf455a64",
   "metadata": {},
   "source": [
    "#### Using a Classification Head\n",
    "\n",
    "The classification head is a specific architecture added on top of the pre-trained model to adapt it for a classification task. \n",
    "The `ClassificationModel` class from the `simpletransformers` library wraps the pre-trained RoBERTa model with a custom classification head suitable for multi-class or binary classification tasks.\n",
    "\n",
    "By default, this head usually consists of one or more fully connected (dense) layers followed by an activation function \n",
    "(like softmax for multi-class classification or sigmoid for binary classification) that maps the model’s output \n",
    "(which is of size equal to the hidden states of the base model) to the number of classes.\n",
    "\n",
    "In this case, since the labels are binary (0 and 1), the classification head is likely configured to output a \n",
    "single logit (or two logits if using softmax) corresponding to the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f3c277c-1f6b-47bd-a041-1b0cecf28172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38a9744c-530d-4569-beab-09edae517ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\simpletransformers\\classification\\classification_model.py:610: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef613be29ce046999607925b29e334cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab40e20756045ec940b5eca1430433f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c89c92a33074a26917bc61e73d7654b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 3:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38c1255ccdb49c49ab47b276722b167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 3:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e528d696ed9489cb661de5f96addd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 3:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 22s\n",
      "Wall time: 4min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21, 0.5545366051651183)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "# takes a long time on CPUs\n",
    "# 5 mins for 100 samples\n",
    "\n",
    "model.train_model(train_df.sample(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9b98a32-4505-4f51-a557-33c7fdbfabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1453: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2351b0e91e849a9a9d73259e402026b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae84c4e357f543a296ea8f3b717ea830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict unseen instances\n",
    "eval_df_samples = eval_df.sample(5)\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcd831cc-98ed-4871-81ff-b352c82f965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model_outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee3cdf87-051b-44bd-ad53-19c181fe719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.30      0.50      0.38         5\n",
      "weighted avg       0.36      0.60      0.45         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\ANACONDA\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\ANACONDA\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming eval_df.label contains the true labels and y_pred contains the predicted labels\n",
    "print(classification_report(eval_df_samples.label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fd314-8654-4b99-8673-2bf656179b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
