{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497db21d-4e08-42a7-9258-90fecced929f",
   "metadata": {},
   "source": [
    "#### Using Embeddings in RAG : Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5725255c-8e4e-459b-ab12-29fca393b662",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372ca766-d5c2-409e-9385-1c1b2b775f86",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import AutoTokenizer, \\\n",
    "                         DPRContextEncoder, DPRQuestionEncoder\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23226224-1ca8-4ab3-b8b6-ffd560b205e4",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(features):\n",
    "    norms               = np.linalg.norm(features, axis=1, keepdims=True)\n",
    "    normalized_features = features / norms\n",
    "    \n",
    "    similarity_matrix         = np.inner(normalized_features, normalized_features)\n",
    "    rounded_similarity_matrix = np.round(similarity_matrix, 4)\n",
    "    \n",
    "    return rounded_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a86800-66b5-4c28-9bfb-ffe8bc215474",
   "metadata": {},
   "source": [
    "#### Pure similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "answers = [\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "    \"The tallest mountain in the world is Mount Everest.\",\n",
    "    \"Mount Shasta\",\n",
    "    \"I like my hike in the mountains\",\n",
    "    \"I am going to a yoga class\"\n",
    "]\n",
    "\n",
    "question = 'What is the tallest mountain in the world?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23055d3e-3403-44bf-b7bf-997b94c0746a",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.7976, 0.4001, 0.3559, 0.0972]\n",
      "Question = What is the tallest mountain in the world?\n",
      "Best answer = What is the tallest mountain in the world?\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "question_embedding = list(model.encode(question))\n",
    "\n",
    "sim = []\n",
    "for answer in answers:\n",
    "    answer_embedding = list(model.encode(answer))\n",
    "    sim.append(cosine_similarity_matrix(np.stack([question_embedding, answer_embedding]))[0,1])\n",
    "\n",
    "print(sim)\n",
    "best_inx = np.argmax(sim)\n",
    "print(f\"Question = {question}\")\n",
    "print(f\"Best answer = {answers[best_inx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42995790-5b01-4105-abeb-670846767af1",
   "metadata": {},
   "source": [
    "#### Dual-Encoder inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f2f60e-7dbc-4df7-8773-aaa9aeb35a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c88a3c-fd44-4ec2-b22f-98687672e314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc46114672284263a0ff8b67f970bc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7214734ebf74bf28efad0f13bb810af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f622971c6534b5a8451b1046ac9947b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model     = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-multiset-base', \n",
    "                                             cache_dir = r'D:\\AI-DATASETS\\07-Hugging-Face-Data')\n",
    "\n",
    "tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-multiset-base', \n",
    "                                             cache_dir = r'D:\\AI-DATASETS\\07-Hugging-Face-Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11abd4e-8b95-4776-bb00-5b54ccd18566",
   "metadata": {},
   "source": [
    "`Encoding Passages:`\n",
    "Use the model to encode text passages into dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae2372f1-bf76-4dcc-9b7d-ca9ab2261613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode a passage\n",
    "passage = \"This is an example passage.\"\n",
    "inputs  = tokenizer(passage, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "outputs    = model(**inputs)\n",
    "embeddings = outputs.pooler_output  # Dense vector representation of the passage\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5b354-1d6a-4a21-aea6-67d2a5033bb5",
   "metadata": {},
   "source": [
    "`Retrieval Process:`\n",
    "For retrieval, encode both the query and the passages, then compute similarities to find the most relevant passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "256032b7-dd79-40b3-bbf9-9bcdadf05846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e89f420f-88b3-450f-a125-fe150e26ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a query\n",
    "query = \"What is an example passage?\"\n",
    "query_inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "query_outputs = model(**query_inputs)\n",
    "query_embedding = query_outputs.pooler_output.squeeze().detach().numpy()  # Convert to 1D array\n",
    "\n",
    "# Encode passages\n",
    "passages = [\"This is an example passage.\", \"Another passage here.\"]\n",
    "passage_embeddings = []\n",
    "for passage in passages:\n",
    "    passage_inputs = tokenizer(passage, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    passage_outputs = model(**passage_inputs)\n",
    "    passage_embeddings.append(passage_outputs.pooler_output.squeeze().detach().numpy())  # Convert to 1D arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5003ad1f-de56-4197-94b8-ea30442b1f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8666056 0.7041036]]\n"
     ]
    }
   ],
   "source": [
    "# Convert passage embeddings to a 2D numpy array\n",
    "passage_embeddings = np.array(passage_embeddings)\n",
    "\n",
    "# Compute similarities\n",
    "similarities = cosine_similarity(query_embedding.reshape(1, -1), passage_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44437a08-54c4-46ba-a666-558dde05743a",
   "metadata": {},
   "source": [
    "#### Differences Between DPR and General Dual Encoders\n",
    "\n",
    "**Dual Encoders:**\n",
    "- **General Concept:** Dual encoders use two separate neural network encoders to process two types of text (e.g., queries and documents) into dense vector embeddings. These embeddings are then compared to compute similarities.\n",
    "- **Use Cases:** General dual encoders can be used for a variety of tasks, including retrieval, question-answering, and sentence similarity. The design and efficiency depend on the specific use case and how the encoders and embeddings are managed.\n",
    "- **Training:** Dual encoders can be trained with various loss functions, including contrastive loss, but the effectiveness and efficiency can vary based on the architecture and training data.\n",
    "\n",
    "**Dense Passage Retrieval (DPR):**\n",
    "- **Specific Design for Retrieval:** DPR is a specialized type of dual encoder architecture designed specifically for dense retrieval tasks.\n",
    "- **Optimized Architecture:** DPR uses two separate BERT-based encoders (or similar models) to create dense embeddings for queries and passages. These embeddings are then compared using efficient nearest neighbor search techniques.\n",
    "- **Training Strategy:** DPR employs a contrastive loss function during training, where it focuses on aligning query and passage embeddings to ensure that relevant pairs are close in the vector space while irrelevant pairs are far apart. This training strategy enhances the retrieval quality.\n",
    "- **Indexing and Retrieval:**\n",
    "  - **Approximate Nearest Neighbor Search:** DPR uses approximate nearest neighbor (ANN) search algorithms (like FAISS or HNSW) to efficiently search through large collections of passage embeddings. This allows for fast retrieval even with large-scale datasets.\n",
    "  - **Efficiency:** The use of specialized retrieval libraries and optimized indexing techniques makes DPR faster in retrieving relevant passages compared to some general dual encoder setups.\n",
    "\n",
    "**Why DPR Might Be Faster:**\n",
    "1. **Optimized Indexing:**\n",
    "   - **DPR:** Utilizes efficient indexing techniques (e.g., FAISS, HNSW) for quick retrieval from large-scale datasets. These techniques accelerate the search process by organizing embeddings in a way that allows for fast approximate nearest neighbor queries.\n",
    "   - **General Dual Encoders:** May not always include such optimized indexing or might use simpler similarity search methods that can be slower for large-scale retrieval.\n",
    "\n",
    "2. **Contrastive Training:**\n",
    "   - **DPR:** Specifically trained with contrastive loss to ensure that the query and passage embeddings are highly effective for retrieval. This means the embeddings are fine-tuned for fast and accurate retrieval.\n",
    "   - **General Dual Encoders:** While they can use contrastive loss, the training may not be as specialized for retrieval efficiency as DPR.\n",
    "\n",
    "3. **Specialization:**\n",
    "   - **DPR:** Designed with retrieval as its primary task, including architecture and training optimizations aimed at improving retrieval speed and accuracy.\n",
    "   - **General Dual Encoders:** Might be designed for a range of tasks, which can lead to less optimization for any single task like retrieval.\n",
    "\n",
    "**Summary:**\n",
    "While both DPR and general dual encoders involve computing similarities between queries and documents using dense embeddings, DPR is tailored specifically for efficient dense retrieval. DPR’s speed advantages come from its specialized indexing techniques and training strategies optimized for large-scale retrieval tasks. General dual encoders might not always have these specific optimizations, leading to potential differences in retrieval speed and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77844dd8-e6d3-4f53-91ef-b368a7020475",
   "metadata": {},
   "source": [
    "... back to the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacaaeb4354742858d9d7ba64bee068f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f0d6196639424797b21025ac0a9d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/492 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a92f16b27ce4c329712845710fa2db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4882dd76664f0490f025fb55ea82b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8e73aa14194e54b4bca9fa7910e962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1197ff00a46f420abee07544a420df9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf90238aeb6e4e498e5ddb6352707350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/493 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66771b1f06542678975a66348462591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb9f8bbc13c4aa7bbfc51327157b661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3a313cc32b4ac881219397f0b4c401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "answer_encoder   = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "\n",
    "question_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "question_encoder   = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf89b540-550a-47cb-a4a5-502d6544d949",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07758244127035141, 0.25172561407089233, 0.18663954734802246, 0.22120100259780884, 0.02641526237130165, -0.1578557789325714, 0.32760268449783325, 0.26732853055000305, -0.08503071963787079, 0.12929508090019226] 768\n"
     ]
    }
   ],
   "source": [
    "# Compute the question embeddings\n",
    "question_tokens = question_tokenizer(question, return_tensors=\"pt\")[\"input_ids\"]\n",
    "question_embedding = question_encoder(question_tokens).pooler_output.flatten().tolist()\n",
    "print(question_embedding[:10], len(question_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3b3c8a2-1556-45cc-8db7-d5339c3e686b",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6253, 0.7472, 0.5506, 0.3687, 0.25]\n",
      "Question = What is the tallest mountain in the world?\n",
      "Best answer = The tallest mountain in the world is Mount Everest.\n"
     ]
    }
   ],
   "source": [
    "sim = []\n",
    "for answer in answers:\n",
    "    answer_tokens = answer_tokenizer(answer, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    answer_embedding = answer_encoder(answer_tokens).pooler_output.flatten().tolist() \n",
    "    sim.append(cosine_similarity_matrix(np.stack([question_embedding, answer_embedding]))[0,1])\n",
    "\n",
    "print(sim)\n",
    "best_inx = np.argmax(sim)\n",
    "print(f\"Question = {question}\")\n",
    "print(f\"Best answer = {answers[best_inx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3254ee3-4aef-4dca-802f-334275a06d05",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e1719-dd9b-4130-93f1-8cd2cf97e2d6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bf79d-f72d-43de-87f8-6bd29ef290ea",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7f73-52ff-4676-8ebf-1b607c284830",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
